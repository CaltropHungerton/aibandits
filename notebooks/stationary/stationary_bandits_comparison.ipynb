{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary multi-armed bandit\n",
    "\n",
    "In the stationary case the reward probability $p_{t, l}$ is fixed, hence $p_{t,l} = p_l$, $ \\forall t \\in \\{1, T\\}$. We will consider here the variant of the problem in which all but the best arm have the same reward probability $p=1/2$, and the reward probability of the 'best' arm is set as $p_{max} = p + \\epsilon$, where $\\epsilon \\in \\left(0, \\frac{1}{2}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, nn, ops, vmap, jit\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the path to the code folder\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# import simulator\n",
    "from bandits import sim_fixed_difficulty as simulator\n",
    "\n",
    "# import the learning rule\n",
    "from bandits import learning_stationary as learning\n",
    "\n",
    "# import decision algorithms\n",
    "from bandits import thompson_selection, ots_selection, ucb_selection, bucb_selection, efe_selection, app_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Given the constrain of this investigation to the Bernoulli bandits we will define the observation likelihood as \n",
    "\n",
    "$$ p(o_t|\\vec{\\theta}, a_t) = \\prod_{k=1}^K \\left[ \\theta_k^{o_{t}}\\left( 1- \\theta_k \\right)^{1-o_{t}} \\right]^{\\delta_{a_t, k}}  $$\n",
    "\n",
    "where $a_t$ denotes the agent's choice at trial $t$. Given the Bernoulli likelihoods we will assume that both priors and posterior correspond to conjugate distribution, the Beta distribution. Hence, we can express the prior (before any observation is made) as a product of prior beliefs over different arms\n",
    "\n",
    "$$ p(\\vec{\\theta}) = \\prod_{k=1}^K \\mathcal{Be}(\\theta_k; \\alpha_{0,k}, \\beta_{0,k}) $$\n",
    "\n",
    "where we assume that initial prior (before making any observations) corresponds to a uniform distribution, \n",
    "hence $\\alpha_{0,k}, \\beta_{0,k} = 1, \\forall \\: k$. Conjugacy of the prior, in stationary cases, allows us to define simple update rules corresponding to exact Bayesian inference, as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\alpha_{t, k} &= \\alpha_{t-1, k} + \\delta_{a_t, k} o_t \\\\\n",
    "    \\beta_{t, k} &= \\beta_{t-1,k} + \\delta_{a_t, k} (1-o_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the parameter update is performed only for a selected $k$th arm at trial $t$. Hence, in a sequential inference setup the posterior beliefs $p(\\vec{\\theta}|o_{t:1})$ at trial $t$, become the prior beliefs at the next trial $t+1$, and can be expressed as\n",
    "\\begin{equation}\n",
    "    p(\\vec{\\theta}|o_{t:1}, a_{t:1}) = \\prod_{k=1}^K \\mathcal{Be}\\left(\\theta_k; \\alpha_{t,k}, \\beta_{t,k}\\right) .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "### Thompson sampling\n",
    "\n",
    "This form of action selection algorithm is derived from the i.i.d samples from the reward probability prior\n",
    "at trial $t$, hence\n",
    "\n",
    "$$a_t = \\arg\\max_k \\theta^*_k, \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "An extension of this often found in the literature, specially on dynamic MABs, is called optimistic \n",
    "Thompson sampling, and is defined as \n",
    "\n",
    "$$a_t = \\arg\\max_k \\max(\\theta^*_k, \\mu_{t-1,k}), \\qquad \\theta^*_k \\sim \\mathcal{Be}(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "where the expected reward probability $\\mu_{t-1, k} = \\frac{\\alpha_{t-1,k}}{\\alpha_{t-1,k} + \\beta_{t-1, k} }$\n",
    "constrains the minimal value of the sample from the prior.\n",
    "\n",
    "\n",
    "### Upper confidence bound (UCB)\n",
    "\n",
    "Another classical algorithm of reinforcement learning with a decision rule defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\left\\{ \\begin{array}{cc}\n",
    "        \\arg\\max_k \\left(m_{k, t-1} + \\frac{\\ln t}{n_{k, t-1}} + \\sqrt{\\frac{ m_{k, t-1} \\ln t}{n_{k, t-1}}}\\right) & \\textrm{for } t>K \\\\\n",
    "        t & \\textrm{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "where $m_{k, t-1} = \\frac{\\alpha_{t-1, k} - \\alpha_0}{n_{t-1, k}}$, and $n_{k, t-1} = \\alpha_{t-1,k} + \\beta_{t-1,k} - \\alpha_0 - \\beta_0$. Here, we will also consider a Bayesian variant of the upper confidence bound, in which the best arm is selected as the one with the highest $z$th percentile of posterior beliefs, where the percentile increases over time as $z_t = 1 - \\frac{1}{t} $. Hence, \n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\arg\\max_k CDF^{-1} \\left( z_t, \\alpha_{t-1}^k, \\beta_{t-1}^k \\right).\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the beta distributed beliefs $\\mathcal{Be}\\left(\\alpha, \\beta\\right)$, the inverse cumulative distribution $CDF^{-1}$ corresponds to the inverse incomplete regularised beta function $I^{-1}_z \\left( \\alpha, \\beta \\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active inference\n",
    "\n",
    "The action selection in active inference rest upon the expected free energy $G(\\pi)$ of behavioral policy $\\pi$. Normally, behavioral policies in active inference correspond to a specific sequence of future actions $\\pi = (a_{t}, \\ldots, a_D)$ up to some planning depth $D$. Here we will limit the analysis to a shallow planning depth of $D=1$ hence each policy corresponds to one of the possible choices, that is actions $a_t$.\n",
    "The expected free energy is defined as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G_t(a) & =  D_{KL}\\left(Q(o_t |a)||P(o_t)\\right) + E_{Q(\\vec{\\theta}|a)}\\left[H[o_t|\\vec{\\theta}, a] \\right] \\\\\n",
    "    & = - E_{Q(o_t|a)}\\left[ \\ln P(o_t) +  D_{KL}\\left( Q(\\vec{\\theta}|o_t, a)|| Q(\\vec{\\theta})\\right) \\right]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$$Q(o_t|a) = \\int d \\vec{\\theta} p \\left(o_t|\\vec{\\theta}, a\\right) Q(\\vec{\\theta}),$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ Q(\\vec{\\theta}|o_t, a) \\propto p(o_t|\\vec{\\theta}, a) Q(\\vec{\\theta}).$$\n",
    "\n",
    "Teh quantity $P(o_t)$ denotes prior preferences over outcomes, and here we set it in a way that the agent always prefers rewards (o_t=1) over no-rewards (o_t=0. We can express this as \n",
    "\n",
    "$$ P(o_t) = \\propto e^{o_t \\lambda} e^{-(1-o_t) \\lambda},$$\n",
    "\n",
    "where $\\lambda$ parametrises strengt of preferences of rewards over no-rewards. Given the expected free energy the action selection rule is defined as \n",
    "\n",
    "$$ a_t = \\arg\\min_a G_t(a).$$\n",
    "\n",
    "Given the known functional expressions for the prior expectation $Q\\left(\\vec{\\theta}\\right)$ as a product of Beta distributions, we get the following expressions for expected free energy \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G_t(a) = & - \\lambda( 2 \\cdot  \\mu_{t-1, a} - 1) + \\mu_{t-1, a} \\ln \\mu_{t-1, a} + (1-\\mu_{t-1, a}) \\ln ( 1- \\mu_{t-1, a}) \\\\\n",
    "    & - \\mu_{t-1,a} \\psi(\\alpha_{t-1, a}) - (1 - \\mu_{t-1,a}) \\psi(\\beta_{t-1, a}) + \\psi(\\nu_{t-1,a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    \\approx & -\\lambda(2 \\mu_{t-1, a} - 1) - \\frac{1}{2 \\nu_{t-1, a}} \\equiv \\tilde{G}_t(a)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Interestingly, the approximate form of the expected free energy allows us to express the expected epistemic affordance of each arm as (approximately)\n",
    "    \\begin{equation}\n",
    "    \\begin{split}\n",
    "        E_{Q(o_t|a)}\\left( D_{KL}\\left[Q(\\vec{\\theta}|o_t, a)||Q(\\vec{\\theta})\\right] \\right) &= E_{Q\\left(o_t, \\vec{\\theta}|a\\right)}\\left( \\ln \\frac{Q(\\vec{\\theta}| o_t, a_t)}{Q(\\vec{\\theta})} \\right) \\approx \\frac{1}{2 \\nu_{t-1, a_t}}\n",
    "    \\end{split}\n",
    "    \\end{equation}\n",
    "    \n",
    "Hence, the information gain of a particular choice $a$ and an outcome $o_t$ becomes\n",
    "\n",
    "\\begin{equation}\n",
    "    D_{KL}\\left[Q(\\vec{\\theta}|o_t, a)||Q(\\vec{\\theta})\\right] = \\left\\{ \\begin{array}{ll} - \\ln \\mu_{t-1}^{a} + \\psi(\\alpha_{t-1}^{a}+1) - \\psi(\\nu_{t-1}^{a}+1), & \\textrm{ for } o_t=1 \\\\ - \\ln \\left( 1 - \\mu_{t-1}^{a}\\right) + \\psi(\\beta_{t-1}^{a}+1) - \\psi(\\nu_{t-1}^{a}+1), & \\textrm{ for } o_t = 0 \\end{array} \\right. \\approx \\left\\{ \\begin{array}{ll} \\frac{1}{2\\nu_{t-1}^{a}} \\left( \\frac{1}{\\mu_{t-1}^{a}} - 1\\right), & \\textrm{ for } o_t=1 \\\\ \\frac{1}{2\\nu_{t-1}^{a}} \\left( \\frac{1}{1 - \\mu_{t-1}^{a}} - 1\\right), & \\textrm{ for } o_t = 0 \\end{array}\\right. \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements regret rate estimation over different AI algos difficulty levels\n",
    "def estimate_regret_rate1(Ks, epsilon=.25, steps=18, N=1000):\n",
    "    # number of steps defines the total run time T = 10^n, where n = steps/9 + 2\n",
    "    regret_all = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    lambdas = jnp.arange(.0, 2., .05)\n",
    "    \n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([efe_selection, app_selection], \n",
    "                       ['EFE_K{}'.format(K), 'APP_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            sim = lambda l: simulator(learning, \n",
    "                                      lambda *args: func(*args, lam=l), \n",
    "                                      N=N, steps=steps, K=K, \n",
    "                                      eps=epsilon, \n",
    "                                      seed=_seed[0])\n",
    "            \n",
    "            cum_regret = vmap(lambda l: sim(l))(lambdas)\n",
    "            regret_all[label].append(cum_regret)\n",
    "            \n",
    "    return regret_all\n",
    "\n",
    "regret1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret1[5] = estimate_regret_rate1(Ks, epsilon=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret1[10] = estimate_regret_rate1(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret1[20] = estimate_regret_rate1(Ks, epsilon=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regret_rate2(Ks, epsilon, steps=27, N=1000):\n",
    "    # number of steps defines the total run time T = 10^n, where n = steps/9 + 2\n",
    "    regret_all = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([thompson_selection, ots_selection, ucb_selection, bucb_selection], \n",
    "                       ['TS_K{}'.format(K), 'OTS_K{}'.format(K), 'UCB_K{}'.format(K), 'BUCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "\n",
    "            cum_regret = simulator(learning, \n",
    "                                   func, \n",
    "                                   N=N, \n",
    "                                   steps=steps, \n",
    "                                   K=K, \n",
    "                                   eps=epsilon,\n",
    "                                   seed=_seed[0])\n",
    "                \n",
    "            regret_all[label].append(cum_regret)\n",
    "    \n",
    "    return regret_all\n",
    "\n",
    "regret2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret2[5] = estimate_regret_rate2(Ks, epsilon=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret2[10] = estimate_regret_rate2(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [10, 20, 40, 80]\n",
    "regret2[20] = estimate_regret_rate2(Ks, epsilon=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "* establish functional or numerical relationship between $\\lambda^*$ and $K$, $\\epsilon$.\n",
    "* determine the optimality relation for AI algorithms $\\lambda^* = f(K, \\epsilon)$.\n",
    "* introduce learning of $\\lambda$ taht minimises regret."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "name": "python394jvsc74a57bd05db8fbda02a7898a88e6621329a372c50a0ec1572867faf6b18754639b5c86c3",
   "display_name": "Python 3.9.4 64-bit ('ppl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}