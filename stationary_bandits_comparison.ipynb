{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary multi-armed bandit\n",
    "\n",
    "In the stationary case the reward probability $p_{t, l}$ is fixed, hence $p_{t,l} = p_l$, $ \\forall t \\in \\{1, T\\}$. We will consider here the variant of the problem in which all but the best arm have the same reward probability $p=1/2$, and the reward probability of the 'best' arm is set as $p_{max} = p + \\epsilon$, where $\\epsilon \\in \\left(0, \\frac{1}{2}\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# define the generative process for rewards and outcomes with zero change probability\n",
    "from jax import devices\n",
    "devices(backend='cpu')\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, nn, ops, vmap, jit\n",
    "\n",
    "from environment import generative_process_swtch\n",
    "rho = .0 # change probability\n",
    "log_pj_j = jnp.log(np.array([[1 - rho, rho], [1, 0]]))\n",
    "\n",
    "process = lambda *args: generative_process_swtch(*args, log_pj_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Given the constrain of this investigation to the Bernoulli bandits we will define the observation likelihood as \n",
    "\n",
    "$$ p(o_t|\\vec{\\theta}, k_t) = \\prod_{k=1}^K \\left[ \\theta_k^{o_{t}}\\left( 1- \\theta_k \\right)^{1-o_{t}} \\right]^{\\delta_{k_t, k}}  $$\n",
    "\n",
    "In both cases we can assume that the prior over reward probabilities is given as the product of conjugate priors of the Bernoulli distribution, that is, the Beta distribution. Hence,\n",
    "\n",
    "$$ p(\\vec{\\theta}|k_t) = \\prod_{k=1}^K \\left[ B(\\theta_k; \\alpha_{0,k}, \\beta_{0,k}) \\right]^{\\delta_{k_t, k}}$$\n",
    "\n",
    "where we assume that initial prior (before making any observations) corresponds to a uniform distribution, \n",
    "hence $\\alpha_{0,k}, \\beta_{0,k} = 1, \\forall \\: k$. Conjugacy of the prior allows us to define simple update rules\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\alpha_{t, k} &= \\alpha_{t-1, k} + \\delta_{k_t, k} o_t \\\\\n",
    "    \\beta_{t, k} &= \\beta_{t-1,k} + \\delta_{k_t, k} (1-o_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "where the parameter update is performed only for a selected $k$th arm at trial $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the learning rule\n",
    "from learning_algos import learning_stationary as learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "### Thompson sampling\n",
    "\n",
    "This form of action selection algorithm is derived from the i.i.d samples from the reward probability prior\n",
    "at trial $t$, hence\n",
    "\n",
    "$$a_t = \\arg\\max_k \\theta^*_k, \\qquad \\theta^*_k \\sim Beta(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "An extension of this often found in the literature, specially on dynamic MABs, is called optimistic \n",
    "Thompson sampling, and is defined as \n",
    "\n",
    "$$a_t = \\arg\\max_k \\max(\\theta^*_k, \\mu_{t-1,k}), \\qquad \\theta^*_k \\sim Beta(\\theta_k; \\alpha_{t-1, k}, \\beta_{t-1, k})$$\n",
    "\n",
    "where the expected reward probability $\\mu_{t-1, k} = \\frac{\\alpha_{t-1,k}}{\\alpha_{t-1,k} + \\beta_{t-1, k} }$\n",
    "constrains the minimal value of the sample from the prior.\n",
    "\n",
    "\n",
    "### Upper confidence bound (UCB)\n",
    "\n",
    "Another classical algorithm of reinforcement learning with a decision rule defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    a_t = \\left\\{ \\begin{array}{cc}\n",
    "        \\arg\\max_k \\left(\\mu_{k, t-1} + \\sqrt{\\frac{2 \\ln t}{\\nu_{k, t-1}-2}}\\right) & \\textrm{for } t>K \\\\\n",
    "        t & \\textrm{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_{k, t-1} = \\frac{\\alpha_{t-1, k}}{\\nu_{t-1, k}}$ and $\\nu_{k, t-1} = \\alpha_{t-1,k} + \\beta_{t-1,k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active inference\n",
    "\n",
    "The action selection in active inference rest upon the expected free energy $G(\\pi)$ of behavioral policy $\\pi$. Normally, behavioral policies in active inference correspond to a specific sequence of future actions $\\pi = (a_{t}, \\ldots, a_D)$ up to some planning depth $D$. Here we will limit the analysis to a shallow planning depth of $D=1$ hence each policy corresponds to one of the possible choices, that is actions $a_t$.\n",
    "The expected free energy is defined as \n",
    "\n",
    "$$ G(a_t) = D_{KL}\\left(Q(\\vec{\\theta}, k_t |a_t)||P(\\vec{\\theta}, k_t)\\right) + E_{Q(\\vec{\\theta}, k_{t}|a_t)}\\left[H[o_t|\\vec{\\theta}, k_t] \\right]$$\n",
    "\n",
    "where $P(\\vec{\\theta}, k_t)$ corresponds to a prior preference over hidden states, and $Q(\\vec{\\theta}, k_t |a_t)$ to the action dependent expectation in trial $t$, hence\n",
    "\n",
    "$$Q(\\vec{\\theta}, k_t |a_t) = p(\\theta|k_t, o_{1:t-1}) p(k_t|a_t) $$\n",
    "\n",
    "The expected free energy forms an upper bound on the expected surprisial $S(a_t)$ defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    S(a_t) & =  D_{KL}\\left(Q(o_t |a_t)||P(o_t)\\right) + E_{Q(\\vec{\\theta}, k_{t}|a_t)}\\left[H[o_t|\\vec{\\theta}, k_t] \\right] \\\\\n",
    "    & = - E_{Q(o_t|a_t)}\\left[ \\ln P(o_t) +  D_{KL}\\left( Q(\\vec{\\theta}, k_t|o_t, a_t)|| Q(\\vec{\\theta}, k_t|a_t)\\right) \\right] \\leq G(a_t)\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$$Q(o_t|a_t) = \\sum_{k_t} \\int d \\vec{\\theta} p(o_t|\\vec{\\theta}, k_t) Q(\\vec{\\theta}, k_t|a_t),$$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ Q(\\vec{\\theta}, k_t|o_t, a_t) \\propto p(o_t|\\vec{\\theta}, k_t) Q(\\vec{\\theta}, k_t|a_t).$$\n",
    "\n",
    "We will assume that the agent has no preference between arms, hence $P(k_t) = \\frac{1}{K}$. However, the agent will prefer higher reward probabilities. We can express this as \n",
    "\n",
    "$$ P(\\vec{\\theta}|k_t) \\propto \\prod_k \\theta_k^{\\delta_{k_t, k} \\left( \\alpha-1 \\right)}$$\n",
    "\n",
    "From the joint preference over arms and reward probabilities we can obtain the marginal preference over outcomes as  \n",
    "\n",
    "$$ P(o_t) = \\sum_{k_t} \\int p(o_t|\\vec{\\theta}, k_t) P(\\vec{\\theta}, k_t) d \\vec{\\theta} \\propto e^{o_t \\lambda} e^{-(1-o_t) \\lambda}, \\qquad \\lambda = \\frac{1}{2} \\ln \\alpha $$\n",
    "\n",
    "We will consider two variants of the action selection rule: \n",
    "\n",
    "* one based on the expected free energy and defined as \n",
    "    $$ p(a_t) \\propto e^{- \\gamma G(a_t)},$$\n",
    "\n",
    "* and another based on expected surprisal and defined as\n",
    "\n",
    "    $$ p(a_t) \\propto e^{- \\gamma S(a_t)}.$$\n",
    "\n",
    "The motivation for this differentiation comes from the fact that minimum of the expected free energy will not in general correspond to the minimum of the expected surprisal, hence \n",
    "\n",
    "$$ \\arg\\min_a G(a) \\neq \\arg\\min_a S(a)$$\n",
    "\n",
    "Given the known functional expressions for the prior expectation $Q(\\vec{\\theta}, k_t| a_t)$, and $Q(o_t| a_t)$ prior preferences $P(\\vec{\\theta}, k_t)$, and $P(o_t)$, and observation likelihood $p(o_t|\\vec{\\theta}, k_t)$ we get the following expressions for expected free energy and expected surprisal \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    G(a_t = a) = & - \\ln \\left[ B(\\alpha_{t-1, a}, \\beta_{t-1, a})\\right] \\\\\n",
    "    & + (\\alpha_{t-1, a} - \\alpha - \\mu_{t-1,a}) \\psi(\\alpha_{t-1, a}) \\\\\n",
    "    & + (\\beta_{t-1, a} - 2 + \\mu_{t-1, a}) \\psi(\\beta_{t-1, a}) \\\\ & + (\\alpha + 2 - \\nu_{t-1, a}) \\psi(\\nu_{t-1, a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    S(a_t = a) = & - \\lambda( 2 \\cdot  \\mu_{t-1, a} - 1) + \\mu_{t-1, a} \\ln \\mu_{t-1, a} + (1-\\mu_{t-1, a}) \\ln ( 1- \\mu_{t-1, a}) \\\\\n",
    "    & - \\mu_{t-1,a} \\psi(\\alpha_{t-1, a}) - (1 - \\mu_{t-1,a}) \\psi(\\beta_{t-1, a}) + \\psi(\\nu_{t-1,a}) - \\frac{1}{\\nu_{t-1,a}} \\\\\n",
    "    \\approx & -\\lambda(2 \\mu_{t-1, a} - 1) - \\frac{1}{2 \\nu_{t-1, a}}\n",
    "    \\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import decision algorithms\n",
    "from choice_algos import thompson_selection, ots_selection, ucb_selection, bucb_selection, efe_selection, sup_selection, app_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP\n",
    "def simulator(process, learning, action_selection, N=100, T=1000, K=10, seed=0, eps=.25):\n",
    "    def sim_fn(carry, t):\n",
    "        rng_key, states, prior = carry\n",
    "        \n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        choices = action_selection(t, prior, _rng_key)\n",
    "        \n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        outcomes, states = process(t, choices, states, _rng_key)\n",
    "        posterior = learning(outcomes, choices, prior)\n",
    "                \n",
    "        return (rng_key, states, posterior), choices\n",
    "    \n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    probs = jnp.concatenate([jnp.array([eps + .5]), jnp.ones(K-1)/2.])\n",
    "    states = [probs, jnp.zeros(1, dtype=jnp.int32)]\n",
    "    prior = jnp.ones((N, K, 2))/2\n",
    "    \n",
    "    _, choices = lax.scan(sim_fn, (rng_key, states, prior), jnp.arange(T))\n",
    "    \n",
    "    return choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements regret rate estimation over different AI algos difficulty levels\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange\n",
    "from jax.interpreters import xla\n",
    "import itertools\n",
    "\n",
    "def estimate_regret_rate1(Ks, epsilon, T=10000, N=500):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    gammas = jnp.arange(1., 21., 1.)\n",
    "    lambdas = jnp.arange(.0, 4., .2)\n",
    "    \n",
    "    times = np.arange(1, T+1)[::10, None]\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([efe_selection, sup_selection, app_selection], \n",
    "                       ['EFE_K{}'.format(K), 'SUP_K{}'.format(K), 'APP_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            sim = jit(lambda e, g, l: simulator(process, \n",
    "                                                learning, \n",
    "                                                lambda *args: func(*args, gamma=g, lam=l), \n",
    "                                                N=N, T=T, K=K, \n",
    "                                                eps=e, \n",
    "                                                seed=_seed[0]))\n",
    "            \n",
    "            vals = jnp.array(list(itertools.product(gammas, lambdas)))\n",
    "            \n",
    "            choices = vmap(lambda g, l: sim(epsilon, g, l))(vals[:, 0], vals[:, 1])\n",
    "            regret = np.cumsum((1 - (choices == 0).astype(jnp.float32)) * epsilon, -2)[:, ::10]\n",
    "            mean_reg[label].append(regret/times)\n",
    "                    \n",
    "            xla._xla_callable.cache_clear()\n",
    "                    \n",
    "    np.savez('res_AI_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate1(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_regret_rate2(Ks, epsilon, T=10000, N=500):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    times = np.arange(1, T+1)[::10, None]\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([ots_selection, ucb_selection], \n",
    "                       ['OTS_K{}'.format(K), 'UCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            l1 = label.split('_')[0]\n",
    "            if l1 == 'BUCB':\n",
    "                choices = simulator_bucb(process, \n",
    "                                         learning, \n",
    "                                         func, \n",
    "                                         N=N, T=T, K=K, \n",
    "                                         eps=epsilon,\n",
    "                                         seed=_seed[0])\n",
    "\n",
    "            else:\n",
    "                choices = simulator(process, \n",
    "                                    learning, \n",
    "                                    func, \n",
    "                                    N=N, T=T, K=K, \n",
    "                                    eps=epsilon,\n",
    "                                    seed=_seed[0])\n",
    "\n",
    "            mean_reg[label].append(np.cumsum((1 - (choices == 0).astype(np.float32)) * epsilon, -2)[::10]/times)\n",
    "                    \n",
    "            xla._xla_callable.cache_clear()\n",
    "                    \n",
    "    np.savez('res_RL_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate2(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implements the simulator for POMDP using bucb selection\n",
    "# inverse incomplete beta is not a jax function, so the code has to be in pure python\n",
    "def scan(func, carry, iterate):\n",
    "    saves = []\n",
    "    for n in iterate:\n",
    "        carry, res = func(carry, n)\n",
    "        saves.append(res)\n",
    "    \n",
    "    return np.stack(saves, 0)\n",
    "\n",
    "def simulator_bucb(process, learning, action_selection, N=100, T=1000, K=10, seed=0, eps=.25):\n",
    "    def sim_fn(carry, t):\n",
    "        rng_key, states, prior = carry\n",
    "        \n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        choices = action_selection(t, prior, _rng_key)\n",
    "        \n",
    "        rng_key, _rng_key = random.split(rng_key)\n",
    "        outcomes, states = process(t, choices, states, _rng_key)\n",
    "        posterior = learning(outcomes, choices, prior)\n",
    "                \n",
    "        return (rng_key, states, posterior), choices\n",
    "    \n",
    "    rng_key = random.PRNGKey(seed)\n",
    "    probs = np.concatenate([np.array([eps + .5]), np.ones(K-1)/2.])\n",
    "    states = [probs, np.zeros(1, dtype=np.int32)]\n",
    "    prior = np.ones((N, K, 2))/2\n",
    "    \n",
    "    choices = scan(sim_fn, (rng_key, states, prior), np.arange(T))\n",
    "    \n",
    "    return choices\n",
    "\n",
    "def estimate_regret_rate_bucb(Ks, epsilon, T=10000, N=500):\n",
    "    mean_reg = defaultdict(lambda: [])\n",
    "\n",
    "    seed = random.PRNGKey(100)\n",
    "    times = np.arange(1, T+1)[::10, None]\n",
    "\n",
    "    for i in trange(len(Ks), desc='K loop'):\n",
    "        K = Ks[i]\n",
    "        for func, label in zip([bucb_selection], \n",
    "                       ['BUCB_K{}'.format(K)]):\n",
    "\n",
    "            seed, _seed = random.split(seed)\n",
    "            choices = simulator_bucb(process, \n",
    "                                     learning, \n",
    "                                     func, \n",
    "                                     N=N, T=T, K=K, \n",
    "                                     eps=epsilon,\n",
    "                                     seed=_seed[0])\n",
    "\n",
    "            mean_reg[label].append(np.cumsum((1 - (choices == 0).astype(np.float32)) * epsilon, -2)[::10]/times)\n",
    "    np.savez('res_BUCB_Ks_e{}'.format(int(epsilon * 100)), **mean_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a9b1771dcd460c8b6f24943784db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K loop', max=5.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3c3d6803af4faabdce0abfdd13eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K loop', max=5.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba41b98a95164de0bae90a319d6b8cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='K loop', max=5.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Ks = [5, 10, 20, 40, 80]\n",
    "estimate_regret_rate_bucb(Ks, epsilon=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "* establish functional or numerical relationship between $\\gamma^*$, $\\lambda^*$ and $K$, $\\epsilon$.\n",
    "* determine the optimality relation for AI algorithms $\\gamma^* = f(\\lambda, K, \\epsilon)$? \n",
    "* introduce learning of $\\gamma$ or $\\lambda$. Would the learning find values of $\\lambda$, $\\gamma$ that minimize regret?"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
